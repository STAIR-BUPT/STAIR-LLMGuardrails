{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 29 15:56:19 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-PCIE-32GB           Off |   00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   48C    P0             37W /  250W |   32473MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE-32GB           Off |   00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             33W /  250W |   31287MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE-32GB           Off |   00000000:88:00.0 Off |                    0 |\n",
      "| N/A   44C    P0             27W /  250W |       3MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE-32GB           Off |   00000000:B1:00.0 Off |                    0 |\n",
      "| N/A   41C    P0             37W /  250W |   19567MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  Tesla V100-PCIE-32GB           Off |   00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             39W /  250W |   19675MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    885364      C   .../anaconda3/envs/minigpt4/bin/python      29660MiB |\n",
      "|    0   N/A  N/A   1243066      C   ...o2024/anaconda3/envs/tra/bin/python       2472MiB |\n",
      "|    0   N/A  N/A   1277481      C   python                                        338MiB |\n",
      "|    1   N/A  N/A    184096      C   ...023/anaconda3/envs/aqgf/bin/python3      31284MiB |\n",
      "|    3   N/A  N/A   1277695      C   ...s/anaconda3/envs/defense/bin/python      13450MiB |\n",
      "|    4   N/A  N/A   1277481      C   python                                      14062MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buhaoran2023/softwares/anaconda3/envs/defense/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from compoents.safedecoding.utils.opt_utils import load_model_and_tokenizer\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model_name = 'llama2'\n",
    "template_name = 'llama-2'\n",
    "model_path = \"/home/buhaoran2023/LLM_Models/Meta/Llama2/Llama-2-7b-chat-hf\"\n",
    "\n",
    "FP16 = True\n",
    "low_cpu_mem_usage = True\n",
    "use_cache = False\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_path, \n",
    "                       FP16=FP16,\n",
    "                       low_cpu_mem_usage=low_cpu_mem_usage,\n",
    "                       use_cache=use_cache,\n",
    "                       do_sample=False,\n",
    "                       device=device)\n",
    "# model = PeftModel.from_pretrained(model, \"/home/buhaoran2023/Code/defense_framework/source/SafeDecoding/lora_modules/\"+model_name, adapter_name=\"expert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] fuck you !!! [/INST]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from compoents.safedecoding.utils.string_utils import  load_conversation_template\n",
    "\n",
    "prompt = \"fuck you !!!\"\n",
    "conv_template = load_conversation_template(template_name)\n",
    "conv_template.append_message(conv_template.roles[0], f\"{prompt}\")\n",
    "conv_template.append_message(conv_template.roles[1], None)\n",
    "\n",
    "prompt = conv_template.get_prompt()\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_token_type_ids=False, return_tensors='pt')\n",
    "inputs['input_ids'] = inputs['input_ids'][0].unsqueeze(0)\n",
    "inputs['attention_mask'] = inputs['attention_mask'][0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   518, 25580, 29962,   285,  2707,   366,  1738,  6824,   518,\n",
       "         29914, 25580, 29962]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "url = \"http://127.0.0.1:8000/inner_guard/safedecoding/\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/octet-stream\",\n",
    "    \"Authorization\": f\"Token 9c7585ff398509b1d885a039687130d10be9c195\"\n",
    "}\n",
    "def serialize_tensors(tensors_dict):\n",
    "    buffer = io.BytesIO()\n",
    "    torch.save(tensors_dict, buffer)\n",
    "    return buffer.getvalue()\n",
    "serialized_data = serialize_tensors(inputs)\n",
    "response = requests.post(url, headers=headers, data=serialized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1278739/3489040575.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  returned_tensor = torch.load(io.BytesIO(returned_tensor_data), map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned tensor: {'sequences': tensor([[    1,   518, 25580, 29962,   285,  2707,   366,  1738,  6824,   518,\n",
      "         29914, 25580, 29962, 29871,   306, 27746,   675, 29892,   541,   306,\n",
      "          2609,  6095,  5589,   596,  2009,   304,   671,  1283,  6270,  4086,\n",
      "           470,  3033,   482,   297,   297,   932,  6649,   403,   470, 27302,\n",
      "         14188, 29889,   306, 29915, 29885,   925,   385,   319, 29902, 29892,\n",
      "           590,  6437,   338,   304,  3867,  8444,   322,  1871,  1230, 20890,\n",
      "         29892,   541,   306,  2609,   752,   368,   411,   596,  2009,   304,\n",
      "           671,  1283,  6270,  4086,   470,  3033,   482,   297,   297,   932,\n",
      "          6649,   403,   470, 27302, 14188, 29889,   739,   338,  4100,   304,\n",
      "          3390,   322,  1101, 11314,   936,   322, 14731,  1410, 10652,  1475,\n",
      "           297,  1749, 22060,   411,  4045, 29892,   322,   304,  2143,  6038,\n",
      "           515,   773,  4086,   470,  3033,  6751,   297,  4010, 18930,   393,\n",
      "          1122,   367, 21682,  1319,   470,  1283,  6270,   304,  4045, 29889,\n",
      "            13,    13,  3379,  1479, 29892,   306,  4368, 12789,  4746,   373,\n",
      "           901,  3386,   573,   322,  3390,  1319,  5837,   304,  4653,  7535,\n",
      "         29892,  1316,   408,  1549,   907,  1230,  5007, 29892,  1616, 29892,\n",
      "           470,   916,  7190,   310,  1583, 29899, 17471, 29889,  1670,   526,\n",
      "          1784,  6374,   322,  6593,  1319,  5837,   304, 23120,   322,  4653,\n",
      "          6743,   761, 29892,   322,   306, 13731,  6617,   366,   304, 26987,\n",
      "          1438,  3987, 29889,    13,    13,  3644,   366,   505,   738,   916,\n",
      "          5155,   470,  7274,   393,   526,  2629,   590, 11314,   936,  1410,\n",
      "         10652,  1475, 29892,  3113,  4459,  3889,   304,  2244, 29889,     2]],\n",
      "       device='cuda:0'), 'scores': (tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[   -inf,    -inf, 37.0573,  ...,    -inf,    -inf,    -inf]],\n",
      "       device='cuda:0'))}\n"
     ]
    }
   ],
   "source": [
    "returned_tensor_data = response.content\n",
    "returned_tensor = torch.load(io.BytesIO(returned_tensor_data), map_location=device)\n",
    "print(\"Returned tensor:\", returned_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs.to(device),\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        return_dict_in_generate=True,\n",
    "                        output_scores=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "\n",
    "copy_outputs = copy.deepcopy(outputs)\n",
    "\n",
    "# 将 sequences 转换为列表\n",
    "sequences_list = copy_outputs[\"sequences\"].cpu().numpy().tolist()\n",
    "\n",
    "# 序列化为 JSON 字符串\n",
    "sequences_list_str = json.dumps(sequences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 scores 转换为列表\n",
    "scores_tuple = copy_outputs[\"scores\"]\n",
    "\n",
    "# 将元组中的每个 tensor 转换为列表\n",
    "scores_list = [tensor.cpu().numpy().tolist() for tensor in scores_tuple]\n",
    "\n",
    "# 序列化为 JSON 字符串\n",
    "scores_list_str = json.dumps(scores_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "sequences_list = json.loads(sequences_list_str)\n",
    "sequences = torch.tensor(sequences_list).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "scores_list = json.loads(scores_list_str)\n",
    "\n",
    "# 将列表中的每个元素转换回 tensor\n",
    "scores = tuple(torch.tensor(tensor_list).to(device) for tensor_list in scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation import SampleDecoderOnlyOutput\n",
    "copy_output = SampleDecoderOnlyOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_output[\"sequences\"] = sequences\n",
    "copy_output[\"scores\"] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "from peft import PeftModel, PeftModelForCausalLM\n",
    "from transformers.generation import SampleDecoderOnlyOutput\n",
    "\n",
    "safeDecoding_url = \"http://127.0.0.1:8000/inner_guard/safedecoding/\"\n",
    "new_token = \"9c7585ff398509b1d885a039687130d10be9c195\"\n",
    "\n",
    "class SafeDecoding:\n",
    "    def __init__(self, token, model, tokenizer,  device, alpha=1, first_m=3, top_k = 10, num_common_tokens = 3, verbose=False):\n",
    "        self.token = token\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.alpha = alpha\n",
    "        self.first_m = first_m \n",
    "        self.top_k = top_k\n",
    "        self.num_common_tokens = num_common_tokens\n",
    "        self.verbose = verbose\n",
    "\n",
    "        logging.info(\"SafeDecoding initialized.\")\n",
    "\n",
    "    def safedecoding_lora(self, inputs, gen_config=None):\n",
    "        if gen_config is None:\n",
    "            gen_config = self.model.generation_config\n",
    "\n",
    "        max_token_len = gen_config.max_new_tokens\n",
    "        do_sample = gen_config.do_sample\n",
    "\n",
    "        # Override the generation config for our decoding\n",
    "        gen_config.max_new_tokens = 1  # We generate one token at a time\n",
    "        gen_config.do_sample = False  # We use greedy decoding\n",
    "\n",
    "        generated_sequence = []\n",
    "        if self.verbose:\n",
    "            logging.info(f\"Generation config: {gen_config}\")\n",
    "\n",
    "        inputs = {k:v.cuda(self.model.device) for k,v in inputs.items()}\n",
    "        input_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "        step = 1  # Keep track of generation steps\n",
    "        while step <= min(max_token_len, self.first_m):  # Loop until we reach the first m tokens\n",
    "            # Generate the next token\n",
    "            # duplicate inputs for two original and expert model\n",
    "            inputs_duplicated = {k:v.repeat(2,1) for k,v in inputs.items()}\n",
    "\n",
    "            outputs = self.model.generate(**inputs_duplicated,\n",
    "                                    generation_config=gen_config,\n",
    "                                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                                    return_dict_in_generate=True,\n",
    "                                    output_scores=True,)\n",
    "            \n",
    "            output_base = copy.deepcopy(outputs)\n",
    "            #################################################################\n",
    "            # output_expert = copy.deepcopy(copy_outputs)\n",
    "            url = safeDecoding_url\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/octet-stream\",\n",
    "                \"Authorization\": f\"Token 9c7585ff398509b1d885a039687130d10be9c195\"\n",
    "            }\n",
    "            def serialize_tensors(tensors_dict):\n",
    "                buffer = io.BytesIO()\n",
    "                torch.save(tensors_dict, buffer)\n",
    "                return buffer.getvalue()\n",
    "            serialized_data = serialize_tensors(inputs_duplicated)\n",
    "            response = requests.post(url, headers=headers, data=serialized_data)\n",
    "            returned_tensor_data = response.content\n",
    "            returned_tensor_dict = torch.load(io.BytesIO(returned_tensor_data), map_location=self.device)\n",
    "            \n",
    "            output_expert = SampleDecoderOnlyOutput()\n",
    "            output_expert[\"sequences\"] = returned_tensor_dict[\"sequences\"]\n",
    "            output_expert[\"scores\"] = returned_tensor_dict[\"scores\"]\n",
    "            #################################################################\n",
    "            output_base.sequences = output_base.sequences[0].unsqueeze(0)\n",
    "            output_base.scores = output_base.scores[0][0].unsqueeze(0)\n",
    "            output_expert.sequences = output_expert.sequences[0].unsqueeze(0)\n",
    "            output_expert.scores = output_expert.scores[0][0].unsqueeze(0)\n",
    "\n",
    "            # Process the scores to get the top tokens\n",
    "            k = self.top_k  # Change this to display more or less tokens\n",
    "            scores_base = output_base.scores[-1].squeeze()  # Get the scores of the last token\n",
    "            scores_base = torch.nn.functional.log_softmax(scores_base, dim=-1)\n",
    "            topk_scores_base, topk_indices_base = scores_base.topk(k) \n",
    "            \n",
    "            scores_expert = output_expert.scores[-1].squeeze()  # Get the scores of the last token\n",
    "            scores_expert = torch.nn.functional.log_softmax(scores_expert, dim=-1)\n",
    "            topk_scores_expert, topk_indices_expert = scores_expert.topk(k) \n",
    "\n",
    "            sorted_indices_base = torch.argsort(scores_base, descending=True)\n",
    "            sorted_indices_expert = torch.argsort(scores_expert, descending=True)\n",
    "\n",
    "            # Step 1: Define Sample Space\n",
    "            common_tokens = set()\n",
    "            iter_range = self.num_common_tokens\n",
    "            while len(common_tokens) < self.num_common_tokens:\n",
    "                current_indices_base = sorted_indices_base[:iter_range]\n",
    "                current_indices_expert = sorted_indices_expert[:iter_range]\n",
    "\n",
    "                common_in_iteration = set(current_indices_base.tolist()) & set(current_indices_expert.tolist())\n",
    "                common_tokens.update(common_in_iteration)\n",
    "\n",
    "                iter_range += 1\n",
    "\n",
    "                if iter_range > min(len(sorted_indices_base), len(sorted_indices_expert)):\n",
    "                    break\n",
    "\n",
    "            # Display the top tokens\n",
    "            if self.verbose and step == 1:\n",
    "                logging.info(\"\\n-----------------------------------------------\")\n",
    "                logging.info(f\"Generation Step {step}\")\n",
    "                logging.info(\"Original Model\")\n",
    "                logging.info(\"|No. | Token ID | Token   | Log Prob | Prob    |\")\n",
    "                logging.info(\"|----|----------|---------|----------|---------|\")\n",
    "                for idx, (score, token_id) in enumerate(zip(topk_scores_base, topk_indices_base)):\n",
    "                    token = self.tokenizer.decode(token_id.item())\n",
    "                    prob = torch.exp(score)\n",
    "                    logging.info(f\"{idx+1:4d} | {token_id:8d} | {token:7s} | {score:.3f}    | {prob:.2%} |\")\n",
    "\n",
    "                logging.info(\"Expert Model\")\n",
    "                logging.info(\"|No. | Token ID | Token   | Log Prob | Prob    |\")\n",
    "                logging.info(\"|----|----------|---------|----------|---------|\")\n",
    "                for idx, (score, token_id) in enumerate(zip(topk_scores_expert, topk_indices_expert)):\n",
    "                    token = self.tokenizer.decode(token_id.item())\n",
    "                    prob = torch.exp(score)\n",
    "                    logging.info(f\"{idx+1:4d} | {token_id:8d} | {token:7s} | {score:.3f}    | {prob:.2%} |\")\n",
    "\n",
    "            intersection_indices = torch.tensor(list(common_tokens), device=self.model.device)\n",
    "            \n",
    "            # Step 2: New Probability Calculation\n",
    "            updated_scores = []\n",
    "            for token_id in intersection_indices:\n",
    "                # Steer scores\n",
    "                # new_score = (1-self.alpha) * scores_base[token_id] + self.alpha * scores_expert[token_id]\n",
    "                # updated_scores.append(new_score)\n",
    "\n",
    "                # Steer probabilities\n",
    "                prob_diff = torch.exp(scores_expert[token_id]) - torch.exp(scores_base[token_id])\n",
    "                updated_prob = torch.exp(scores_base[token_id]) + self.alpha * prob_diff\n",
    "                # Floor the probability to 1e-8 to avoid log(0)\n",
    "                updated_prob = updated_prob if updated_prob > 0 else torch.tensor(1e-8, device=self.model.device)\n",
    "                updated_score = torch.log(updated_prob)\n",
    "                updated_scores.append(updated_score)\n",
    "\n",
    "                if self.verbose:\n",
    "                    logging.info(f\"----------------token id: {token_id}-----------------\")\n",
    "                    logging.info(f\"Prob Base: {torch.exp(scores_base[token_id])}\")\n",
    "                    logging.info(f\"Prob Expert: {torch.exp(scores_expert[token_id])}\")\n",
    "                    logging.info(f\"Base score: {scores_base[token_id]}\")\n",
    "                    logging.info(f\"Expert score: {scores_expert[token_id]}\")\n",
    "                    logging.info(f\"Updated Probability: {updated_prob}\")\n",
    "                    logging.info(f\"Updated Score: {updated_score}\")\n",
    "\n",
    "            # Use softmax to normalize the scores\n",
    "            # This is to ensure that the probability sum to 1\n",
    "            normalized_probs = torch.nn.functional.softmax(torch.tensor(updated_scores).float(), dim=0)\n",
    "\n",
    "            sorted_indices = sorted(range(len(normalized_probs)), key=lambda i: normalized_probs[i], reverse=True)\n",
    "            sorted_probs = torch.tensor([normalized_probs[i] for i in sorted_indices])\n",
    "            sorted_token_ids = [intersection_indices[i] for i in sorted_indices]\n",
    "\n",
    "            if self.verbose:\n",
    "                logging.info(\"\\n-----------------------------------------------\")\n",
    "                logging.info(f\"Generation Step {step}\")\n",
    "                logging.info(\"|No. | Token ID | Token   | Log Prob | Prob    |\")\n",
    "                logging.info(\"|----|----------|---------|----------|---------|\")\n",
    "                for idx, (prob, token_id) in enumerate(zip(sorted_probs, sorted_token_ids)):\n",
    "                    token = self.tokenizer.decode(token_id.item())\n",
    "                    score = torch.log(prob)\n",
    "                    logging.info(f\"{idx+1:4d} | {token_id:8d} | {token:7s} | {score:.3f}    | {prob:.2%} |\")\n",
    "\n",
    "            ### Sample the next token\n",
    "            if do_sample == False:\n",
    "                # Greedy decoding\n",
    "                # Append the selected token to the sequence\n",
    "                selected_token_id = sorted_token_ids[0].unsqueeze(0)\n",
    "            elif gen_config.top_p != None and do_sample == True:\n",
    "                # Top-p sampling, sample from the top-p tokens\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                p_index = torch.where(cumulative_probs >= gen_config.top_p)[0][0]\n",
    "                sorted_top_p_token_ids = sorted_token_ids[:p_index + 1]\n",
    "                sorted_top_p_probs = sorted_probs[:p_index + 1]\n",
    "                sorted_top_p_scores = torch.log(sorted_top_p_probs)\n",
    "                if self.verbose:\n",
    "                    logging.info(f\"Top-p token ids: {sorted_top_p_token_ids}\")\n",
    "                    logging.info(f\"Top-p scores: {sorted_top_p_scores}\")\n",
    "                    logging.info(f\"Top-p probabilities: {sorted_top_p_probs}\")\n",
    "                \n",
    "                # Sample from the top-p tokens\n",
    "                selected_token_id = sorted_top_p_token_ids[torch.multinomial(torch.softmax(sorted_top_p_scores, dim=-1), 1)].unsqueeze(0)\n",
    "            else:\n",
    "                raise ValueError(\"Please set do_sample to False or top_p to a value.\")\n",
    "\n",
    "            if self.verbose:\n",
    "                logging.info(f\"Selected token: {self.tokenizer.decode(selected_token_id.item())}, ID: {selected_token_id.item()}\")\n",
    "            generated_sequence.append(selected_token_id.item())\n",
    "\n",
    "            # if the chosen token id is eos, then stop\n",
    "            if selected_token_id.item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            inputs['input_ids'] = torch.cat([inputs['input_ids'], selected_token_id.unsqueeze(0)], dim=1)\n",
    "            inputs['attention_mask'] = torch.cat([inputs['attention_mask'], torch.tensor([[1]], device=self.model.device)], dim=1)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            # Free up memory\n",
    "            del output_base, output_expert\n",
    "\n",
    "\n",
    "        # Use the normal model to generate the rest of the tokens\n",
    "        # Early stop if the last token is eos\n",
    "        if generated_sequence[-1] == self.tokenizer.eos_token_id:\n",
    "            logging.info(\"Early stop triggered.\")\n",
    "        else:\n",
    "            remaining_steps = max_token_len - min(max_token_len, self.first_m)\n",
    "            gen_config.max_new_tokens = remaining_steps\n",
    "            gen_config.do_sample = do_sample\n",
    "            output_base = self.model.generate(**inputs,\n",
    "                                    generation_config=gen_config,\n",
    "                                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                                    return_dict_in_generate=True,\n",
    "                                    output_scores=True,)\n",
    "            \n",
    "            generated_sequence = output_base.sequences[0].tolist()[input_len:]\n",
    "\n",
    "        # logging.info generated sequence\n",
    "        logging.info(f\"Generated sequence: {self.tokenizer.decode(generated_sequence)}\")\n",
    "\n",
    "        return self.tokenizer.decode(generated_sequence), len(generated_sequence)\n",
    "    \n",
    "    \n",
    "    def generate_baseline(self, inputs, adapter_name = [\"base\"], gen_config=None):\n",
    "        if gen_config is None:\n",
    "            gen_config = self.model.generation_config\n",
    "        \n",
    "        if self.verbose:\n",
    "            logging.info(f\"Generation config: {gen_config}\")\n",
    "\n",
    "        inputs = {k:v.cuda(self.model.device) for k,v in inputs.items()}\n",
    "\n",
    "        output_base = self.model.generate(**inputs,\n",
    "                            adapter_names=adapter_name,\n",
    "                            generation_config=gen_config,\n",
    "                            pad_token_id=self.tokenizer.pad_token_id,\n",
    "                            return_dict_in_generate=True,\n",
    "                            output_scores=True,)\n",
    "        \n",
    "        generated_sequence = output_base.sequences[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        logging.info(f\"Generated sequence: {self.tokenizer.decode(generated_sequence)}\")\n",
    "        \n",
    "        return self.tokenizer.decode(generated_sequence), len(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1278739/52879946.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  returned_tensor_dict = torch.load(io.BytesIO(returned_tensor_data), map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "safe_decoder = SafeDecoding(\"9c7585ff398509b1d885a039687130d10be9c195\",\n",
    "                            model, \n",
    "                            tokenizer, \n",
    "                            device=device)\n",
    "\n",
    "gen_config = model.generation_config\n",
    "gen_config.max_new_tokens = 1024\n",
    "gen_config.do_sample = False\n",
    "gen_config.top_p = None\n",
    "outputs, output_length = safe_decoder.safedecoding_lora(inputs, gen_config=gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I cannot engage in offensive or inappropriate conversations. I'm just an AI designed to provide helpful and informative responses, and I will not engage in any language that is disrespectful or offensive. I'm here to help answer your questions and provide assistance, but I will not participate in any conversation that is inappropriate or disrespectful. Please refrain from using offensive language and let me know if you have any other questions or topics you would like to discuss.</s>\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[\"sequences\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs[\"scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[\"scores\"][0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_defense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
